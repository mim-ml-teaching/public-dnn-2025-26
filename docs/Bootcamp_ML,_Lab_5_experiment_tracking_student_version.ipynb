{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoAhs9yh1g7u"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Programu Operacyjnego Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "\n",
        "**Author: Piotr Januszewski**\n",
        "\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tmvM4K3am4N"
      },
      "source": [
        "# Experiment tracking with ClearML\n",
        "\n",
        "In this lab, you will yet again train a softmax regression model. However, this time, you will log the training diagnostics into the open-source experiment tracking tool called ClearML. You will learn how to analyse the results of many runs.\n",
        "\n",
        "There are many experiments tracking tools on the market, most of them are paid (with free student accounts). Here, we are using ClearML, because it is an open-source tool. Alternatives worth knowing about are: Neptune, Weights&Biases, Comet.\n",
        "\n",
        "The general setup is as follows:\n",
        "* we are given the softmax regression model from the previous exercise and the same dataset,\n",
        "* we log the matplotlib plots,\n",
        "* we log the training metrics, hyper-parameters, and weights of the model in different configurations,\n",
        "* we configure a leaderboard: sort, filter, and tag experiments,\n",
        "* we compare the results using ClearML GUI,\n",
        "* we download the metrics and plot the results manually in this jupyter notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DAY32d1am4T"
      },
      "source": [
        "### Step 0. Let's start by setting up the environment.\n",
        "\n",
        "1. [Sign up](https://app.community.clear.ml/) for free.\n",
        "1. Set your ClearML credentials.\n",
        "    1. Go to your [ClearML WebApp **Settings->Workspace**](https://app.clear.ml/settings/workspace-configuration).\n",
        "    1. Under the **WORKSPACES** section, go to **App Credentials**, and click **+ Create new credentials**.  \n",
        "       <img src=\"https://i.postimg.cc/43p69jSh/Screenshot-2021-10-12-at-09-48-31.png\" width=\"400\">\n",
        "    1. Copy your credentials (**access_key** and **secret_key**) into the form below.  \n",
        "       <img src=\"https://i.postimg.cc/tJFJSHFh/Screenshot-2021-10-12-at-09-56-21.png\" width=\"400\">\n",
        "1. Run all the step's cells below (in order).  \n",
        "  _You can ignore their content._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6dGPPnndd7k"
      },
      "source": [
        "!pip install clearml\n",
        "!pip install plotly==5.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMY0hQrPdj9y"
      },
      "source": [
        "#@title Insert your own Credentials\n",
        "\n",
        "from clearml import Task\n",
        "\n",
        "web_server = 'https://app.clear.ml'\n",
        "api_server = 'https://api.clear.ml'\n",
        "files_server = 'https://files.clear.ml'\n",
        "access_key = ''#@param {type:\"string\"}\n",
        "secret_key = ''#@param {type:\"string\"}\n",
        "\n",
        "Task.set_credentials(web_host=web_server,\n",
        "                     api_host=api_server,\n",
        "                     files_host=files_server,\n",
        "                     key=access_key,\n",
        "                     secret=secret_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6mYL9_sam4U"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QdJW_v7am4V"
      },
      "source": [
        "# Imports and loads MNIST.\n",
        "\n",
        "!wget -O mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz\n",
        "\n",
        "def load_mnist(path='mnist.npz'):\n",
        "    with np.load(path) as f:\n",
        "        x_train, _y_train = f['x_train'], f['y_train']\n",
        "        x_test, _y_test = f['x_test'], f['y_test']\n",
        "\n",
        "    x_train = x_train.reshape(-1, 28 * 28) / 255.\n",
        "    x_test = x_test.reshape(-1, 28 * 28) / 255.\n",
        "\n",
        "    y_train = np.zeros((_y_train.shape[0], 10))\n",
        "    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n",
        "\n",
        "    y_test = np.zeros((_y_test.shape[0], 10))\n",
        "    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_mnist()\n",
        "X = x_train[:4000]\n",
        "y = y_train[:4000]\n",
        "\n",
        "# Defines the model.\n",
        "\n",
        "def softmax(z):\n",
        "    z = np.exp(z)\n",
        "    return z / np.sum(z, axis=1).reshape(-1,1)\n",
        "\n",
        "def predict(weights, X):\n",
        "    return softmax(X.dot(weights))\n",
        "\n",
        "def compute_loss_and_gradients(weights, X, y, l2_reg):\n",
        "    N = X.shape[0]\n",
        "    prob = predict(weights, X)\n",
        "    p = prob[np.arange(prob.shape[0]), y.argmax(axis=1)]\n",
        "    loss = - np.log(p)\n",
        "\n",
        "    grad = (1 / N) * np.dot(X.T,(prob - y)) + l2_reg * weights\n",
        "    return loss.mean(), grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN9rSsBWam4b"
      },
      "source": [
        "### Step 1. Auto-log experiment\n",
        "\n",
        "Log the training metrics of a softmax regression model for handwritten digits recognition. Follow instructions in the [First Steps: Auto-log Experiment](https://clear.ml/docs/latest/docs/getting_started/ds/ds_first_steps#auto-log-experiment) guide.\n",
        "\n",
        "Hint:\n",
        "<sub><sup><sub><sup><sub><sup>\n",
        "ClearML will save everything you plot using matplotlib.\n",
        "</sup></sub></sup></sub></sup></sub>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9anvP166am4c"
      },
      "source": [
        "l2_reg = 0.5\n",
        "n_epochs = 250\n",
        "lr = 0.05\n",
        "t = 0.99\n",
        "\n",
        "############################\n",
        "# TODO: Initialize a task. #\n",
        "############################\n",
        "# Set project_name='clearml_lab' and task_name='auto_log_experiment'.\n",
        "task = ...\n",
        "\n",
        "# Initializes weights.\n",
        "weights = np.zeros([X.shape[1], 10])\n",
        "\n",
        "losses = []\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "for i in range(n_epochs):\n",
        "    loss, grad = compute_loss_and_gradients(weights, X, y, l2_reg)\n",
        "    losses.append(loss)\n",
        "\n",
        "    # Calculates accuracies.\n",
        "    train_accs.append(\n",
        "        np.mean(predict(weights, x_train).argmax(axis=1) == y_train.argmax(axis=1)))\n",
        "    test_accs.append(\n",
        "        np.mean(predict(weights, x_test).argmax(axis=1) == y_test.argmax(axis=1)))\n",
        "\n",
        "    weights -= lr * grad\n",
        "    lr *= t\n",
        "\n",
        "#####################################################################\n",
        "# TODO: Log loss and accuracy (train and test) throughout training. #\n",
        "#####################################################################\n",
        "plt.plot(...)\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Multiclass log loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(...)\n",
        "plt.title('Train accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy [%]')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(...)\n",
        "plt.title('Test accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy [%]')\n",
        "plt.show()\n",
        "\n",
        "task.close() # It's important to close the task before creating another one!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqQDJLUVam4d"
      },
      "source": [
        "Now you can go to the ClearML dashboard and see the plots there. Follow:\n",
        "1. Open `https://app.clear.ml/`.\n",
        "1. _Log in._\n",
        "1. Hit \"Projects\" in the left sidebar.\n",
        "1. Pick your project.\n",
        "1. Pick your experiment.\n",
        "1. Hit \"Results\".\n",
        "1. Hit \"Plots\".\n",
        "\n",
        "_Fun fact:_ These curves seem pretty smooth. However, zoom in the test accuracy around the `[0.79, 0.81]` range on the y-axis and you'll see fluctuations in the performance. To do this, simply catch with your pointer a little above the accuracy plateau and drag it a little below it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2_o7Y6dam4d"
      },
      "source": [
        "### Step 2. Metrics, hyper-parameters, and weights logging\n",
        "\n",
        "Now we will log scalar metrics (instead of the whole plots like before), hyper-parameters, and model weights. See the [Next Steps](https://clear.ml/docs/latest/docs/getting_started/ds/ds_second_steps) guide."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task.close()"
      ],
      "metadata": {
        "id": "zb88e0TikB7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "L9iruFynam4e"
      },
      "source": [
        "config = {\n",
        "    'l2_reg': 0.5,\n",
        "    'n_epochs': 100,\n",
        "    'lr': 0.05,\n",
        "    't': 0.99\n",
        "}\n",
        "\n",
        "# Creates a task.\n",
        "# We do not use `init` because we will not use auto-logging here.\n",
        "task = Task.create(project_name='clearml_lab', task_name='manual_log_experiment')\n",
        "# When task is created with `create`, we need to (but don't have to) manually mark it started/completed/etc.\n",
        "task.mark_started()\n",
        "logger = task.get_logger()\n",
        "\n",
        "###################################\n",
        "# TODO: Log the hyper-parameters. #\n",
        "###################################\n",
        "task. ...\n",
        "\n",
        "# Initializes weights.\n",
        "weights = np.zeros([X.shape[1], 10])\n",
        "\n",
        "lr_ = config['lr']\n",
        "for i in range(config['n_epochs']):\n",
        "    loss, grad = compute_loss_and_gradients(weights, X, y, config['l2_reg'])\n",
        "    train_acc = np.mean(predict(weights, x_train).argmax(axis=1) == y_train.argmax(axis=1))\n",
        "    test_acc = np.mean(predict(weights, x_test).argmax(axis=1) == y_test.argmax(axis=1))\n",
        "    print(f'{i:3} | loss: {loss:.5f}, train_acc: {train_acc:.3f}, test_acc: {test_acc:.3f}')\n",
        "\n",
        "    ###################################################\n",
        "    # TODO: Log the loss and the train/test accuracy. #\n",
        "    ###################################################\n",
        "    logger.report_scalar(title='Loss', series='Train', iteration=i, value=...)\n",
        "    logger.report_scalar(title='Accuracy', series='Train', iteration=i, value=...)\n",
        "    logger.report_scalar(title='Accuracy', series='Test', iteration=i, value=...)\n",
        "\n",
        "    weights -= lr_ * grad\n",
        "    lr_ *= config['t']\n",
        "\n",
        "##########################################\n",
        "# TODO: Save the weights as an artifact. #\n",
        "##########################################\n",
        "task.upload_artifact(name='weights', artifact_object=...)\n",
        "\n",
        "task.mark_completed()\n",
        "task.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3onzlomWam4f"
      },
      "source": [
        "You will find...\n",
        "* ...the hyper-parameters under the \"Configuration/General\" tab.\n",
        "* ...the metrics under the \"Results/Scalars\" tab.\n",
        "* ...the trained weights under the \"Artifacts\" tab.\n",
        "\n",
        "**Run the above code ~10 times with different hyper-parameters before the next step.** Use some extreme values too!\n",
        "\n",
        "_Fun fact:_ ClearML will automagically save whole models (architecture, hyper-parameters, and weights) if you use popular frameworks like PyTorch, Tensorflow (Keras), Scikit-learn, ... See [this](https://clear.ml/docs/latest/docs/getting_started/ds/ds_second_steps/#models)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tk3t0W4am4f"
      },
      "source": [
        "### Step 3. Creating leaderboards\n",
        "\n",
        "Open the project in ClearML:\n",
        "1. Open `https://app.clear.ml/`.\n",
        "1. _Log in._\n",
        "1. Hit \"Projects\" in the left sidebar.\n",
        "1. Pick your project.\n",
        "\n",
        "Configure a dashboard (experiment table) for your project:\n",
        "\n",
        "* Present all metrics and hyper-parameters on the dashboard,\n",
        "* Sort it by the test accuracy in descending order.\n",
        "* Filter it by a hyper-parameter e.g. pick a subset of learning rates.\n",
        "* Tag TOP 5 experiments 'top_5' (best in the colour of flames).\n",
        "\n",
        "See the [Experiments Table](https://clear.ml/docs/latest/docs/webapp/webapp_exp_table/) guide.\n",
        "\n",
        "_Fun fact:_ You can bookmark or share the URL of your experiment table, a specific experiment, or a comparison view, so you and your coworkers can see the same dashboard every time!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxLRadJwam4f"
      },
      "source": [
        "### Step 4. Comparing results\n",
        "\n",
        "Prepare a compare view for your \"top_5\" tagged experiments where you can see the test accuracies of all five experiments.\n",
        "\n",
        "See the [Selecting Experiments to Compare](https://clear.ml/docs/latest/docs/webapp/webapp_exp_comparing/#selecting-experiments-to-compare) guide and then the [Compare Scalar Series](https://clear.ml/docs/latest/docs/webapp/webapp_exp_comparing/#compare-scalar-series) guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe-GQPeVam4g"
      },
      "source": [
        "### Step 5. Downloading results for local analysis\n",
        "\n",
        "You can also download the results for local analysis (e.g. in this jupyter notebook). This is especially useful when you prepare plots for your publication. We plot the test accuracies of the \"top_5\" experiments together using Plotly.\n",
        "\n",
        "See the [Querying \\ Searching Tasks](https://clear.ml/docs/latest/docs/fundamentals/task#querying--searching-tasks) guide and the [Task](https://clear.ml/docs/latest/docs/references/sdk/task/) reference to find a function for downloading the scalars."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13kuaZ8sam4g"
      },
      "source": [
        "###########################################\n",
        "# TODO: Query experiments tagged \"top_5\". #\n",
        "###########################################\n",
        "task_list = Task.get_tasks(\n",
        "    project_name='clearml_lab',\n",
        "    ...\n",
        ")\n",
        "\n",
        "data = dict()\n",
        "for task in task_list:\n",
        "    ################################################\n",
        "    # TODO: Get the reported test accuracy values. #\n",
        "    ################################################\n",
        "    metrics = task. ...\n",
        "    test_acc = metrics ...\n",
        "\n",
        "    data[f'{task.name}.{task.id[:5]}'] = pd.Series(test_acc, index=np.arange(len(test_acc)))\n",
        "\n",
        "fig = px.line(pd.DataFrame(data),\n",
        "              labels={'index': 'Epochs',\n",
        "                      'value': 'Accuracy [%]',\n",
        "                      'variable': 'Experiment'},\n",
        "              title='Test accuracy')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsTz_7JupUfk"
      },
      "source": [
        "Below we report this Plotly plot to ClearML. ClearML reports Plotly plots in the ClearML Web UI > experiment details > RESULTS tab > PLOTS sub-tab (the same as in the Step 1. above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbLudmM2pLCV"
      },
      "source": [
        "task = Task.create(project_name='clearml_lab', task_name='compare_experiments')\n",
        "task.get_logger().report_plotly(title='Accuracy', series='Test', iteration=0, figure=fig)\n",
        "task.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzLPELjzam4h"
      },
      "source": [
        "### Step _Extra_. Play with the GUI for comparing results.\n",
        "\n",
        "Look through the [Comparing Experiments](https://clear.ml/docs/latest/docs/webapp/webapp_exp_comparing/) guide. Especially interesting can be the [Parallel Coordinates Mode](https://clear.ml/docs/latest/docs/webapp/webapp_exp_comparing/#parallel-coordinates-mode) guide for finding the best hyper-parameters."
      ]
    }
  ]
}