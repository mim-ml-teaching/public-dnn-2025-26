{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Points: 20\n",
    "\n",
    "You will be working with RGBNet: a network that accepts pixel position as input and outputs a triplet with R, G, B channels of that pixels.\n",
    "RGBNet is trained on a fixed image. Your tasks are:\n",
    "\n",
    "1. (14 points) Fill gaps in the code, which creates embeddings in 2 ways:\n",
    "    - Learned embedding of size 64 (7 points)\n",
    "    - Positional embedding of size 64 (7 points)\n",
    "\n",
    "\n",
    "Please note that your code should train within 1 minute and report training loss below 15 for each case.\n",
    "2. (6 points) Visualize output of the network for each encoding. Does it resemble the input image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import urllib\n",
    "from typing import Literal\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "IMG_URL = \"https://i.natgeofe.com/k/8fa25ea4-6409-47fb-b3cc-4af8e0dc9616/red-eyed-tree-frog-on-leaves-3-2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_response = urllib.request.urlopen(IMG_URL)\n",
    "img = cv2.imdecode(np.array(bytearray(url_response.read()), dtype=np.uint8), -1)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "img = cv2.resize(img, (0,0), fx=0.01, fy=0.01) \n",
    "im_w, im_h = img.shape[0], img.shape[1]\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveEncoding(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 64)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x.float())\n",
    "\n",
    "\n",
    "class LearnedEncoding(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        # Your code goes here. Output dim of embedding should be 64 \n",
    "        ...    \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Your code goes here. Output dim of embedding should be 64 \n",
    "        ...\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):    \n",
    "    def __init__(self) -> None:\n",
    "        # Your code goes here. Output dim of embedding should be 64 \n",
    "        ...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Your code goes here. Output dim of embedding should be 64 \n",
    "        ...\n",
    "\n",
    "\n",
    "# Define the network\n",
    "class RGBNet(nn.Module):\n",
    "    def __init__(self, encoding_type: Literal[\"naive\", \"learned\", \"positional\"]) -> None:\n",
    "        super().__init__()\n",
    "        if encoding_type == \"naive\":\n",
    "            self.encoding = NaiveEncoding()\n",
    "        elif encoding_type == \"learned\":\n",
    "            self.encoding = LearnedEncoding() \n",
    "        elif encoding_type == \"positional\":\n",
    "            self.encoding = PositionalEncoding()\n",
    "        else:\n",
    "            raise ValueError(\"Wrong encoding type!\")\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 3)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoding(x)\n",
    "        x = F.softplus(self.fc1(x))\n",
    "        x = F.softplus(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(used_embedding: Literal[\"naive\", \"learned\", \"positional\"]) -> torch.nn.Module:\n",
    "    # Instantiate the model and set it to the GPU (if available)\n",
    "    model = RGBNet(encoding_type=used_embedding)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss(reduction=\"mean\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Define the number of epochs and batch size\n",
    "    num_epochs = 300\n",
    "    batch_size = 32\n",
    "\n",
    "    X, y = torch.cartesian_prod(torch.tensor(range(im_w)), torch.tensor(range(im_h))).to(device), torch.flatten(torch.tensor(img, dtype=torch.float32), start_dim=0, end_dim=1).to(device)\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        perm = torch.randperm(X.size(0))\n",
    "        X, y = X[perm,:], y[perm, :]\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            # Get the current batch\n",
    "            X_batch = X[i:i+batch_size]\n",
    "            y_batch = y[i:i+batch_size]\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/X.size(0)}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_output(model: RGBNet) -> None:\n",
    "    # Your code goes here. Visualize the predicted image from pixels\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: \n",
    "# training code works only for \n",
    "# used_embedding = \"naive\"\n",
    "# training and visualization code should work in both\n",
    "# used_embedding = \"learned\"\n",
    "# used_embedding = \"positonal\"\n",
    "used_embedding = \"naive\"\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = train(used_embedding=used_embedding)\n",
    "visualize_model_output(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fa2fa4f4d9d3d9ca73eb3739cc0e85a72773041ed8c7376d5dc2c41e6946bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
