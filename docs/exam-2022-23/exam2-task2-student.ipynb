{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb47428",
   "metadata": {},
   "source": [
    "You are given stochastic policy $\\pi(a|s)$. In reinforcement learning, a stochastic policy is a probability distribution over possible actions that an agent can take in a particular state. Unlike a deterministic policy, which selects a single action with the highest probability, a stochastic policy allows the agent to select multiple actions with varying probabilities. \n",
    "\n",
    "The stochastic policy guides the agent as he interacts with a predefined MDP environment. The policy is disturbed with a \"$\\epsilon$-greedy\" like process - with probability $\\epsilon$ agent performs a random action (i.e. sampled from uniform distribution) and with probability 1 - $\\epsilon$ agent performs a policy action (i.e. sampled from $\\pi(a|s)$). \n",
    "\n",
    "The environment has 200 different states and 5 allowed actions. Furthermore, environment has deterministic transitions and a fixed starting state:\n",
    "\n",
    "$$ P(s'|s,a) = 1 \\quad \\forall (s, a) $$\n",
    "\n",
    "Your task is to write a function which, given the interactions with the environment, outputs the log-likelihood of the trajectory. Please bear in mind the epsilon-greedy disturbance!\n",
    "\n",
    "## Hint:\n",
    "\n",
    "Let $s_i, a_i$ be a random variables indicating state and action samples at step $i \\in [0,t]$. The log-likelihood we are looking for is equal to:\n",
    "\n",
    "$$ L = \\log \\bigl(P(s_0) * P(a_0|s_0) * P(s_1|s_0, a_0) * P(a_1|s_1) \\  *\\  ...\\  *\\  P(s_{t}|s_{t-1}, a_{t-1}) * P(a_t|s_t)\\bigr) $$\n",
    "\n",
    "Since starting state is fixed and transitions are deterministic, $P(s_0)$ and $P(s_{i+1}|s_i, a_i)$ can be neglected. However, note that $P(a_i|s_i)$ is affected by the $\\epsilon$-greedy disturbance. Let E be a random variable with the following distrubution:\n",
    "\n",
    "$$ P(E = 1) = \\epsilon \\quad \\text{and} \\quad P(E = 0) = 1 - \\epsilon $$\n",
    "\n",
    "E represents the epsilon greedy disturbance. As such, given 5 possible actions in each state we have:\n",
    "\n",
    "$$ P(a|s, E=1) = \\frac{1}{5} \\quad \\text{and} \\quad P(a|s, E=0) = \\pi(a|s)$$\n",
    "\n",
    "Try to first calculate $P(a|s)$ using law of total probability!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53714b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "TIMESTEPS = 100\n",
    "N_STATES = 200\n",
    "N_ACTIONS = 5\n",
    "EPSILON = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f1534",
   "metadata": {},
   "source": [
    "We define the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d024ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self):\n",
    "        self.transitions = np.random.choice(np.arange(N_STATES), size=(N_STATES, N_ACTIONS))\n",
    "        self.rewards = np.random.randint(0,10, size=(N_STATES, N_ACTIONS))\n",
    "\n",
    "    def reset(self, starting_state=0):\n",
    "        state = starting_state\n",
    "        self.state = state\n",
    "        self.step_idx = 0\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.step_idx += 1\n",
    "        new_state = self.transitions[self.state, action]\n",
    "        reward = self.rewards[self.state, action]\n",
    "        self.state = new_state\n",
    "        terminal = True if self.step_idx > TIMESTEPS else False\n",
    "        return new_state, reward, terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33ad7aa",
   "metadata": {},
   "source": [
    "The policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b268e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    def __init__(self):\n",
    "        logits = np.random.uniform(1,10, size=(N_STATES, N_ACTIONS))        \n",
    "        self.probabilities = np.exp(logits / 0.4) / np.exp(logits / 0.4).sum(1, keepdims=True)\n",
    "        \n",
    "    def get_action_and_probs(self, state):\n",
    "        epsilon = np.random.uniform(0,1)\n",
    "        if epsilon > EPSILON:\n",
    "            action = np.random.choice(np.arange(N_ACTIONS), p=self.probabilities[state])\n",
    "        else:\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "        return action, self.probabilities[state] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2254ba50",
   "metadata": {},
   "source": [
    "And a function that generates the trajectory data for $t$ timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ce26233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory_data():\n",
    "    np.random.seed(1)\n",
    "    actions = np.zeros(TIMESTEPS).astype(np.int16)\n",
    "    policy = Policy()\n",
    "    env = Env()\n",
    "    policy_matrix = policy.probabilities\n",
    "    transition_matrix = env.transitions\n",
    "    state = env.reset()\n",
    "    for i in range(TIMESTEPS):\n",
    "        action, probs = policy.get_action_and_probs(state)\n",
    "        new_state, reward, terminal = env.step(action)\n",
    "        actions[i] = action\n",
    "        state = new_state\n",
    "        if terminal:\n",
    "            state = env.reset()\n",
    "    return actions.astype(np.int16), policy_matrix, transition_matrix\n",
    "\n",
    "actions, policy_matrix, transition_matrix = get_trajectory_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babb3ec8",
   "metadata": {},
   "source": [
    "Lets investigate the arrays returned by **get_trajectory_data**. First there is *actions*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3833d8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 0 1 0 0 1 3 1 2 4 3 4 4 0 0 2 4 4 4 0 0 2 4 4 4 0 0 2 2 4 1 4 1 4\n",
      " 4 1 3 2 0 2 2 4 3 3 2 1 2 1 1 0 2 4 4 3 4 1 0 4 4 4 3 3 4 4 0 0 4 0 1 3 3\n",
      " 1 3 2 4 3 2 1 4 4 1 3 2 3 3 4 1 1 2 1 2 1 2 1 4 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bfacb5",
   "metadata": {},
   "source": [
    "*Actions* array shows which action were performed at timestep $i$ (e.g. agent performed actions[0] in timestep 0). There are 100 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c95c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00108576 0.99882944 0.00000009 0.00008223 0.00000248]\n",
      " [0.00004114 0.00034041 0.01226394 0.03881531 0.9485392 ]\n",
      " [0.00003235 0.01286576 0.00000026 0.98710162 0.        ]\n",
      " [0.92229417 0.00309783 0.07457955 0.0000061  0.00002234]\n",
      " [0.02003422 0.8683322  0.00000035 0.00174708 0.10988615]\n",
      " [0.59156613 0.00000001 0.         0.00000005 0.40843381]\n",
      " [0.         0.00000567 0.99741475 0.00007057 0.00250901]\n",
      " [0.00000714 0.03011679 0.84377811 0.00000001 0.12609795]\n",
      " [0.98458086 0.0043779  0.00000012 0.01104112 0.        ]\n",
      " [0.00003149 0.99996665 0.00000098 0.00000086 0.00000002]]\n"
     ]
    }
   ],
   "source": [
    "print(policy_matrix[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ffca6b",
   "metadata": {},
   "source": [
    "Furthermore, there is *policy_matrix*. The array is of size (N_STATES x N_ACTIONS) and shows $\\pi(a|s)$ for particular state-action pair (e.g. the policy probability of performing action \"1\" in state \"0\" ($\\pi(a_1|s_0)$) is equal to policy_matrix[0,1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a72acb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61 167 134 135 140]\n",
      " [115 128   3  51 143]\n",
      " [195  81  59  66 143]\n",
      " [ 61 192  19 172  87]\n",
      " [ 61  21 120 128  63]\n",
      " [ 94 129  84 188  22]\n",
      " [142 152 174 104  32]\n",
      " [198  54 152 163  57]\n",
      " [159  39  14 113 176]\n",
      " [110  13  19 143  97]]\n"
     ]
    }
   ],
   "source": [
    "print(transition_matrix[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06945112",
   "metadata": {},
   "source": [
    "Finally, there is *transition_matrix*. The array is of size (N_STATES x N_ACTIONS) and shows the new state after performing certain action in specific state (e.g. after performing action \"1\" in state \"0\" the agent will be transitioned to state transition_matrix[0,1]). Note the deterministic transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a95e1",
   "metadata": {},
   "source": [
    "Complete the function that calculates log-likelihood of the sampled trajectory. You should first find which states were visited (using the starting state, *actions* and *transition_matrix*) and later find probability values that we are interested in (using the sequance of visited states, *actions* and *policy_matrix*).\n",
    "\n",
    "1. **Agent starts in state \"0\" (i.e. the first row of policy_matrix and transition_matrix). Remember to keep the starting state in your list of visited states!**\n",
    "2. **Keep in mind that *policy_matrix* does not incorporate the $\\epsilon$-greedy!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2937fa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def calculate_loglikelihood(actions, policy_matrix, transition_matrix):\n",
    "    ## YOUR CODE HERE ##\n",
    "    return None\n",
    "\n",
    "print(calculate_loglikelihood(actions, policy_matrix, transition_matrix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
