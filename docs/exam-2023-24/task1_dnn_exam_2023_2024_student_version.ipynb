{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 (6p)\n",
        "Your task is to modify the custom implementation of MultiHeadAttention. This custom implementation, currently, enables each token to attent to every other token.\n",
        "\n",
        "\n",
        "Your job is to change this behavior in a specific way.\n",
        "Let $S$ be our input sequence of length $2 \\cdot k$:\n",
        "- tokens on positions $i \\lt k$ should attend to prefix of $S$ of length $k$ ($S[:k]$) - every token up to position k\n",
        "- tokens on positions $i \\ge k$ should attend to prefix of $S$  of length $i + 1$ ($S[:i + 1]$) - every previous token and itself\n",
        "\n",
        "(Note: You can assume the sequence length is always an even number)."
      ],
      "metadata": {
        "id": "VFiew9xpybfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_head):\n",
        "      super().__init__()\n",
        "      self.d_model = d_model\n",
        "      self.num_heads = num_heads\n",
        "      self.d_head = d_head\n",
        "\n",
        "      self.W_Q = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
        "      self.W_K = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
        "      self.W_V = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
        "      self.W_O = torch.nn.Linear(num_heads*d_head, d_model, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      seq_len, batch_size, _ = x.shape\n",
        "\n",
        "      Q = self.W_Q(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
        "      K = self.W_K(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
        "      V = self.W_V(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
        "\n",
        "      scaled_QK = torch.einsum(\"ibhd,jbhd->bhij\", Q, K) / math.sqrt(self.d_head)\n",
        "      # shape of scaled_QK is (batch_size, num_heads, seq_len, seq_len)\n",
        "      #TODO\n",
        "\n",
        "\n",
        "      #ENDTODO\n",
        "      weights = F.softmax(scaled_QK, -1)\n",
        "      attention = torch.einsum(\"bhij,jbhd->ibhd\", weights, V)\n",
        "\n",
        "      result = self.W_O(attention.reshape(seq_len, batch_size,self.num_heads * self.d_head))\n",
        "\n",
        "      return result, weights"
      ],
      "metadata": {
        "id": "ENFiK_cTeDM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your solution\n",
        "d_model =\n",
        "num_heads=\n",
        "d_head =\n",
        "k =\n",
        "batch_size =\n",
        "\n",
        "mha = MultiHeadAttention(d_model, num_heads, d_head)\n",
        "batched_x= torch.randn((2*k, batch_size, d_model))\n",
        "with torch.no_grad():\n",
        "  result, weights = mha(batched_x)\n",
        "print(\"Result:\", result)\n",
        "print(\"Weights:\", weights)"
      ],
      "metadata": {
        "id": "GDQ0a57NeB-z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}