{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SMeFV5OsjCW"
      },
      "source": [
        "# The Transformer\n",
        "In this lab scenario, you will implement *causal attention* for a transformer decoder model.\n",
        "The transformer architecture was introduced in the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper, and has dominated the field of language modeling.  \n",
        "Here we will go through different parts of the transformer architecture and explain each of them briefly.\n",
        "\n",
        "The whole notebook works fine on Colab CPU (~5min).\n",
        "On the other hand, it's instantaneous on GPU (except for downloading the weights, which can take most of the time anyway).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDvzsLpSsjCW"
      },
      "source": [
        "## Transformer Overview\n",
        "\n",
        "Transformer decoder models (such as LLaMa 3.1 and Mistral) are popular text-processing models.   \n",
        "\n",
        "One can distinguish two versions of such models: **base** and **instruction-tuned**. The base models are usually trained on predicting the continuation of a given text (for each prefix they output a probability distribution over the next text fragment). In contrast, the instruction-tuned ones are base models that were additionally fine-tuned to follow instructions (often with a form of reinforcement learning from human feedback to generated text ([RLHF](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM6J3vJhsjCW"
      },
      "source": [
        "### Tokenizer\n",
        "\n",
        "The text is presented to the transformer as a sequence of **tokens**.  \n",
        "Tokens are integers used to represent pieces of text.  \n",
        "To be more precise: to convert text to tokens, we first prepare a dictionary of common text fragments.   \n",
        "We usually want to have all possible letters in this dictionary, so that all texts can be tokenized.   \n",
        "We then assign to each text piece from the dictionary an integer and use the dictionary to convert text into a sequence of tokens (integers).  \n",
        "The class that converts text into tokens is called a tokenizer.  \n",
        "\n",
        "In this lab scenario, we will use the OpenLLaMAv2 tokenizer and the HuggingFace library to tokenize text.   \n",
        "HuggingFace contains a vast collection of transformer model weights and implementations along with training and inference code.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XbhhbGQri3qp"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers==4.57.2  # This is the version this notebook was prepared for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Bsmn2wWyUUo"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from functools import partial\n",
        "from pprint import pp\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "from transformers.masking_utils import create_causal_mask\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Suppress a useless warning from HuggingFace.\n",
        "warnings.filterwarnings(\"ignore\", message=\"(?s:.)*authentication is recommended but still optional to access public models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339,
          "referenced_widgets": [
            "121242101dbb4d5a916141261e628e24",
            "c61fe28ec75641728a14c6eb3e801f88",
            "b9a1441b49944668a68fe99a2525e179",
            "fafb3c99c32849bdbf72fbb22b7dd040",
            "f41439fafbf14d1d9700103b17e81bfc",
            "0a0f0c5153184ad880e9f3a7e9e73db3",
            "233484b793c54580beadbd2de3f70a77",
            "4d94f63790374018beb0fa1b2f6574e8",
            "2d58fd285f20425abfcd3bf3fb70f750",
            "9fcb83f2d7844e5084015534f9637e29",
            "3c2874a63b974260af0c475c68ec7454",
            "4d7f54182f954482a06c6896b473ec57",
            "cddf6548f7804406bf16a9c6347ba7ec",
            "3494526be30b4fe097394bb59327bbfa",
            "c1bc1d18e850403e810e69ecb9056036",
            "a1d1956311704903aa36e4ab311f20d8",
            "da0c39d06a8b49278737657170426c50",
            "ea2f71e364504613995053a38ed40e91",
            "f9465c12c2e24315a897589fcebd2f88",
            "d56fe6d7726f4424b81b093fba1c2193",
            "bbcbd0ab1b064884a20cd0befb91a600",
            "ba58c07d42f442a39dc919361785713a",
            "42bd8db552c14bdc9fa4fcc67d58d897",
            "aa199aa606f9409b945c713a6245f877",
            "b120125199f34dd5a0a76fa4b82efde2",
            "febba3d44af346e79283f777d1019a84",
            "7aef77dc7fd84783a9943412fd03830a",
            "bb8b5e9a223d4bc38f9da8879f2dd369",
            "ef3bcaea2f6e4bd59ed1dc7964949efb",
            "746f297169f54c84a8f419be84601f52",
            "449af9c589a742c2a5d50231c91dc2a9",
            "f97e2c5b61724b72878ed866331e524b",
            "a702e0c4226f4078a9cbce43bd7c1d0c"
          ]
        },
        "id": "NbUYzMugsjCX",
        "outputId": "d89c3400-a926-43ae-9997-77905cd96c2f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "121242101dbb4d5a916141261e628e24",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/593 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d7f54182f954482a06c6896b473ec57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/512k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42bd8db552c14bdc9fa4fcc67d58d897",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/330 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [1, 660, 325, 371, 1938, 1880, 347, 389, 477, 8206, 753],\n",
            " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "['<s>',\n",
            " 'This',\n",
            " 'is',\n",
            " 'an',\n",
            " 'example',\n",
            " 'text',\n",
            " 'that',\n",
            " 'we',\n",
            " 'will',\n",
            " 'token',\n",
            " 'ize']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(\"openlm-research/open_llama_3b_v2\", legacy=False)\n",
        "\n",
        "text = \"This is an example text that we will tokenize\"\n",
        "tokens_mask = tokenizer(text)\n",
        "pp(tokens_mask)\n",
        "\n",
        "detokenized = tokenizer.batch_decode(tokens_mask[\"input_ids\"])\n",
        "pp(detokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnkzqaZssjCY"
      },
      "source": [
        "After tokenization the the HuggingFace tokenizer returns a sequence of tokens (`input_ids`) and information on whether the model should look at the ith element of the input (`attention_mask`).  \n",
        "The other part is useful when we want to tokenize several sequences into one batch of elements of the same length. Then the attention mask can be used to hide the padding from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN5jyhaAsjCZ",
        "outputId": "fb668bb2-b86c-4774-e826-c33ccea140be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[   1,  660,  325,  371, 1938, 1880,  347,  389,  477, 8206,  753],\n",
            "        [   1, 8479,    2,    2,    2,    2,    2,    2,    2,    2,    2]]),\n",
            " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "['<s> This is an example text that we will tokenize',\n",
            " '<s> Hello</s></s></s></s></s></s></s></s></s>']\n"
          ]
        }
      ],
      "source": [
        "text = [\"This is an example text that we will tokenize\", \"Hello\"]\n",
        "\n",
        "# We set the padding token to be the same as the end-of-sequence token (EOS).\n",
        "# The EOS token (</s> in this case) can mark the end of the sequence in training and can be used by a model to indicate it finishes its response.\n",
        "# The BOS token (here <s>) can be used to mark the beginning of the input.\n",
        "# Details vary between different models and implementations.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tokens_mask = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=False)\n",
        "pp(tokens_mask)\n",
        "\n",
        "detokenized = tokenizer.batch_decode(tokens_mask[\"input_ids\"])\n",
        "pp(detokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ac-0ziMsjCZ"
      },
      "source": [
        "### Embedding\n",
        "The input to the model is a batch of integer (token) sequences of shape `(batch, seq_len)` where:\n",
        "* `batch` is the size of the batch;\n",
        "* `seq_len` is the length of the longest input sequence inside the batch (the attention mask is used to mask the padding).\n",
        "\n",
        "The first layer of the model replaces each integer with an embedding vector of length `hidden_size`.  \n",
        "(Inside the model, there is a matrix of trainable parameters, randomly initialized, of shape `(num_dictionary_elements, hidden_size)`).  \n",
        "\n",
        "After the embedding step, we pass a tensor of shape `(batch, seq_len, hidden_size)` through the remaining layers of the model.\n",
        "In decoder-only models, the sequence length does not change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0qpsi1lsjCZ"
      },
      "source": [
        "### Transformer layer\n",
        "The internal parts of the transformer are grouped into transformer layers.  \n",
        "Usually, each layer consists of: layer norm, attention, layer norm, and a feed-forward layer.  \n",
        "To be more precise the computation progresses roughly as presented below:\n",
        "```python\n",
        "def transformer_layer(x):\n",
        " x = attention(layer_norm_attn(x)) + x\n",
        " x = feed_forward(layer_norm_ff(x)) + x\n",
        " return x\n",
        "```\n",
        "Here:  \n",
        "* **feed_forward** is an MLP (typically just two layers: linear-activation-linear) that acts on each token independently, in the same way. That is, it treats the sequence-length dimension like the batch dimension, and operates on the `hidden_size` dimension of an input of shape `(batch, seq_len, hidden_size)`.\n",
        "* **layer_norm** – in LLaMa models replaced by [RMSNorm](https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html) (which are like LayerNorm, but without centering, i.e. without subtracting the mean). Similarly as `feed_forward` it operates only on the `hidden_size` dimension, treating other dimensions as a batch.\n",
        "* **attention** – causal multi-head attention that you will implement in further parts of this notebook.\n",
        "\n",
        "Let $t^{(1)}$ be an input tensor of shape `(batch, seq_len, hidden_size)`.\n",
        "Attention will output a tensor $t^{(2)}$ of the same shape with the following property:\n",
        "calculation of $t^{(2)}[b,s,h]$ depends only on values from $t^{(2)}[b,s',h']$ such that  `s' <= s`. In other words, calculation is done independently per batch entry and dependency is *causal* (the past can influence the future but the future cannot influence the past of the sequence)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf4Nv-uVsjCa"
      },
      "source": [
        "### LM head\n",
        "In the end, a linear projection is used to create weights for each element of the input dictionary.\n",
        "To be more precise we take a tensor of shape `(batch, seq_len, hidden_dim)` and use norm + a linear projection from `hidden_dim` to `vocab_size`, in order to change it into tensor of shape `(batch, seq_len, vocab_size)`.  \n",
        "Then we apply softmax over the last dimension (`vocab_size`) to get a probability distribution, for each element of the sequence.\n",
        "The training loss will be cross entropy over next-token prediction.  \n",
        "That is, for a tokenized input sequence $x$, the $i$-th output of the model (which only depends on $x[0], \\dots, x[i]$), should be $x[i+1]$.\n",
        "In other words, the ground-truth output is the input sequence shifted by one (with EOS added at the end)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dea7kXYmsjCa"
      },
      "source": [
        "### Example\n",
        "Below we show the steps described above using OpenLLaMAv2 3B.\n",
        "\n",
        "For full details, see: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShxFREWlsjCa",
        "outputId": "9afdb292-ce5b-45d9-978f-b439bc182195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[    1, 29500, 29536,   835, 29500, 29574,   419, 29500],\n",
            "        [    1, 29500, 29536, 29589, 29574, 29554,    13,     2]]),\n",
            " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 0]])}\n"
          ]
        }
      ],
      "source": [
        "## Tokenize the input.\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"openlm-research/open_llama_3b_v2\", legacy=False)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "text = [\"2 + 7 = \", \"2+7=\"]\n",
        "\n",
        "tokens_mask = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "tokens = tokens_mask[\"input_ids\"]\n",
        "attention_mask = tokens_mask[\"attention_mask\"]\n",
        "pp(tokens_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "0b2e4406f72544c49a8162a02722b036",
            "571f927ce00048b39f93ff66cb1650b8",
            "56ddf54c44fa4ed4bbedaea20f4469ed",
            "9668b954f2f6425d8af7de098668337d",
            "d7a03094c33946a7aa768ea97c49554d",
            "fdbfd060124a4e4cba0b0edab7998335",
            "20b8c554c7cd4923becf79b25a758698",
            "2ca543c9ea164c3e8c03b8c95b811152",
            "fe89e90e845b4b4fbf82db7c76dc2e56",
            "b0cd837e91e944baba370468fa49a546",
            "c73f70acf9464789afffac3deb802a7e",
            "d76da3d1c641423386e07706651530dc",
            "6dd4cf578ec84204b5594cfcc53ca842",
            "7809a55457be44069e00b396d2de0de6",
            "cb270aa625ab434c81960508b0784101",
            "f8f457cd3f194f0ab7e7877b7765e112",
            "a093a97e06bf47e3a01e683e91872616",
            "42425ad1fe4848c2afbed028ad85b2db",
            "b17ed32e8a864eac876a27610e02e1af",
            "d44c9322296f412b96a30838f1737635",
            "595b326ba9d243f98d394bd37c64191e",
            "dcd4ecd78169415daf5b1a717b4729d1",
            "634ab167538647138052cba71a0e2033",
            "f366096ef30b4dad9e1a9e19cced3982",
            "6545e23d361d4ecebdfd76f4f283f74a",
            "0d67f7366a6145ffa86d0efc0c41c17d",
            "69358a3c707c4e2c92109ff55e5c8e20",
            "d4b2a0a360fd46428f35c238a2b5a2ca",
            "3c30abb7ad6c44b686b93053ff347919",
            "10363274817149aca6f8e8ccedf2a74f",
            "33a2faef6711498e92223f6e3b3cd8ce",
            "75ff215af8254347b1282f9a4631647a",
            "484bca20577e4ddc8c71f63b480d3a4f",
            "026db10dacf344248d81b572a4d2b530",
            "45548b4c209e46229c5ec747b25500aa",
            "38d3ea381c38400f8471d08b526c03b0",
            "069d0b2e5b7247638e403da5e51abbc1",
            "57c74c2e03d343d79f133ff8a99a7aa2",
            "f8c61d2c67dd42e6aec0feda54e99011",
            "414f00ac924c4a70a1cce7007fcbb6d0",
            "34c725f90b3e4002929b04d08a0cc3c8",
            "25be06f983ad4881af2617d7f7f5ccfd",
            "d34507ebfbef4212a0a664fc3c2cca19",
            "fca860d1169948bba9cf891de1ab3fe1"
          ]
        },
        "id": "Gs7MxcEvsjCa",
        "outputId": "1c9354f2-cea4-4823-a65c-5a45be6bda09"
      },
      "outputs": [],
      "source": [
        "## Load the model from HuggingFace.\n",
        "device = (\n",
        "    torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        ")\n",
        "\n",
        "# Downloads ~6.8GB in bfloat16 (2 bytes per parameter).\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    \"openlm-research/open_llama_3b_v2\", dtype=torch.bfloat16, device_map=device\n",
        ")\n",
        "model.eval()\n",
        "print(sum(p.numel() for p in model.parameters()) * 2 / 1000 / 1000 / 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbxluG6-sjCb",
        "outputId": "45170f80-a522-461b-fb36-94a82372390b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding(32000, 3200, padding_idx=0)\n",
            "tokens.shape=torch.Size([2, 8])\n",
            "embedded_tokens.shape=torch.Size([2, 8, 3200])\n"
          ]
        }
      ],
      "source": [
        "## Embed the tokenizer input.\n",
        "print(model.model.embed_tokens)\n",
        "with torch.no_grad():\n",
        "    embedded_tokens = model.model.embed_tokens(tokens.to(device))\n",
        "print(f\"{tokens.shape=}\\n{embedded_tokens.shape=}\")\n",
        "batch, seq_len, hidden_size = embedded_tokens.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CqNRwX5y9TAb"
      },
      "outputs": [],
      "source": [
        "## Take the positions of each token [0, 1, 2, ...]\n",
        "position_ids = torch.arange(seq_len, device=embedded_tokens.device)[None, ...]\n",
        "\n",
        "# Embed them as (cos, sin, ...) rotations.\n",
        "# In Llama, instead of concatenating these to the embedded input tokens,\n",
        "# they will be applied to keys and queries inside each attention layer.\n",
        "# ( RoPE: https://arxiv.org/pdf/2104.09864 )\n",
        "# (The first tensor here is only used for its .device and .dtype).\n",
        "with torch.no_grad():\n",
        "    position_embeddings = model.model.rotary_emb(torch.zeros_like(embedded_tokens), position_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF9cPSHwBD9Q",
        "outputId": "a9c74868-2167-4d89-c75f-cf5e2cf148f0"
      },
      "outputs": [],
      "source": [
        "# Compute the causal mask\n",
        "# (including information about the attention_mask from padding).\n",
        "causal_mask = create_causal_mask(\n",
        "    config=model.config,\n",
        "    input_embeds=embedded_tokens,\n",
        "    attention_mask=attention_mask,\n",
        "    position_ids=position_ids,\n",
        "    past_key_values=None,\n",
        "    cache_position=position_ids.squeeze(),\n",
        ")\n",
        "print(causal_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "DU9i_Uw3sjCb",
        "outputId": "3a0ae198-91a5-4e0d-b314-c3d7098b5369"
      },
      "outputs": [],
      "source": [
        "## Go through the layers of the model.\n",
        "x = embedded_tokens\n",
        "\n",
        "with torch.no_grad():\n",
        "    for layer in tqdm(model.model.layers):\n",
        "        x = layer(\n",
        "            x,\n",
        "            attention_mask=causal_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=None,  # can be used to continue generation\n",
        "            use_cache=False,\n",
        "            cache_position=position_ids.squeeze(),\n",
        "            position_embeddings=position_embeddings,\n",
        "        )\n",
        "\n",
        "    x = model.model.norm(x)\n",
        "    x = model.lm_head(x)\n",
        "    x = torch.nn.functional.softmax(x, dim=-1)\n",
        "    print(f\"{x.shape=}\")\n",
        "    next_token = torch.argmax(x[:, -1], dim=-1)\n",
        "    print(next_token)\n",
        "    print([tokenizer.decode(t) for t in next_token])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "M5stJOy0sjCb",
        "outputId": "7b231b38-e8be-49c5-be3a-f4742764692b"
      },
      "outputs": [],
      "source": [
        "# Using HuggingFace generate().\n",
        "with torch.no_grad():\n",
        "    text = \"The largest animal on earth is\"\n",
        "    tokens_mask = tokenizer(text, return_tensors=\"pt\")\n",
        "    output = model.generate(\n",
        "        inputs=tokens_mask[\"input_ids\"].to(device),\n",
        "        max_new_tokens=8,\n",
        "        num_beams=1,\n",
        "        num_return_sequences=4,\n",
        "        do_sample=True, # sample from the distribution created by softmax\n",
        "        temperature=0.7, # divide pre softmax score by this value\n",
        "        top_p=0.9 # cut out improbable tokens from sampling\n",
        "    )\n",
        "    pp(tokenizer.batch_decode(output))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH4maqpOdHKS"
      },
      "source": [
        "## Tools for implementing attention\n",
        "\n",
        "#### `einsum`\n",
        "[torch.einsum](https://docs.pytorch.org/docs/stable/generated/torch.einsum.html) is a useful tool for computing various forms of *contractions*, that is, expressions like $\\sum_i A_{\\dots,i,\\dots} B_{\\dots,i,\\dots}$.\n",
        "For example:\n",
        "* `torch.einsum(\"ij,jk->ik\", A, B)` is matrix multiplication.\n",
        "* `torch.einsum('bij,bjk->bik', As, Bs)` is batched matrix multiplication.\n",
        "\n",
        "In general, you go over every tuple (b,i,j,k,…) of all occuring letters, multiply the specified left and specified right element and accumulate that into the specified target element.\n",
        "In other words, you sum over dimension that don't appear in output.\n",
        "\n",
        "#### `where`\n",
        "Use [torch.where(B,X,Y)](https://docs.pytorch.org/docs/stable/generated/torch.where.html) to implement an expression like $\\begin{cases}\n",
        "    X_{i,j} & \\text{if }B_{i,j}\\\\\n",
        "    Y_{i,j} & \\text{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "#### `tril`\n",
        "[torch.tril(A, diagonal=d)](https://docs.pytorch.org/docs/stable/generated/torch.tril.html) returns the lower-diagonal part of a matrix (not necessarily square), obtained be zeroing-out everything strictly above the main diagonal.\n",
        "If $d \\neq 0$ is given, everything above the diagonal `main + d` is zeroed out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyvy9AU_sjCb"
      },
      "source": [
        "## Causal Attention Implementation\n",
        "Your task is to finish the implementation of the attention mechanism below. In case of problems, you can refer to the original implementation that can be found [here](https://github.com/huggingface/transformers/blob/7f95372c6267d3163fd2aa74aeff9d84ddb6cc35/src/transformers/models/llama/modeling_llama.py#L258).\n",
        "\n",
        "To be more precise, you are given query,key,value tensors (with positional encoding already applied), each of shape:\n",
        ">`(batch, seq_len, num_heads, head_size)`  \n",
        "\n",
        "Your task is to compute for each head a scaled dot product between each query and each key that is either at the same position as the query or precedes the query in the sequence.\n",
        "That is, calculate a tensor `a` of shape:\n",
        "> `(batch, num_heads, seq_len, seq_len)`\n",
        "\n",
        "where  \n",
        "$$\n",
        "    a[b, h, q, k]=\n",
        "\\begin{cases}\n",
        "    \\sum_{d}{\\mathrm{query}[b, q, h, d] \\cdot \\mathrm{key}[b, k, h, d]} / \\sqrt{\\mathrm{head\\_size}}, & \\text{if }k \\leq q\\\\\n",
        "     -\\mathrm{large\\_number},              & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Then you should calculate the softmax over the last dimension of `a` creating `p`.  \n",
        "\n",
        "$$p = \\mathrm{SoftMax}(a)$$\n",
        "Then you should calculate\n",
        "$$ out [b, q, h, d] = \\sum_{k}{ p [b, h, q, k] \\cdot \\mathrm{value} [b, k, h, d] } $$\n",
        "\n",
        "That is, for each query you should gather the `value`s using the probability distribution defined by `p`.  \n",
        "In the end, you should reshape `out` to\n",
        "> `(batch, seq_len, num_heads * head_size)`\n",
        "\n",
        "and apply a linear projection `output_projection`.  \n",
        "\n",
        "To compute the attention mask, use `tril` on `torch.ones((?, ?), device=x.device, dtype=torch.bool)`.\n",
        "For simplicity, you may assume that the number of queries is equal to the number of keys.  \n",
        "This is not always true: for example, when we run `generate()` from HuggingFace transformers library, it caches previous keys and values and create queries only for the new token(s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8WKIblFsjCb"
      },
      "outputs": [],
      "source": [
        "def attention_forward(\n",
        "    query: Tensor, key: Tensor, value: Tensor, output_projection: torch.nn.Linear,\n",
        ") -> Tensor:\n",
        "    batch, q_seq_len, num_heads, head_dim = query.shape\n",
        "    batch, k_seq_len, num_heads, head_dim = key.shape\n",
        "\n",
        "    assert value.shape == key.shape\n",
        "\n",
        "    assert q_seq_len <= k_seq_len\n",
        "    assert query.shape[0] == key.shape[0]\n",
        "    assert query.shape[2:] == key.shape[2:]\n",
        "\n",
        "    # TODO {\n",
        "\n",
        "    # Dot products of every query with every key\n",
        "\n",
        "    # Causality: set upper diagonal part to -infty.\n",
        "\n",
        "\t# Softmax for each query (over keys).\n",
        "\n",
        "\t# Collect the values for each query, as sum over keys/values weighted by attention.\n",
        "\n",
        "    # Reshape and apply the output projection.\n",
        "\n",
        "    # TODO }\n",
        "    assert out.shape == (batch, q_seq_len, num_heads * head_dim)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsxhAVLNsjCb"
      },
      "source": [
        "### Integration with OpenLLaMA\n",
        "The code below integrates your solution from above into OpenLLaMA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pMdJ87CAsjCb"
      },
      "outputs": [],
      "source": [
        "# Copied from https://github.com/huggingface/transformers/blob/7f95372c6267d3163fd2aa74aeff9d84ddb6cc35/src/transformers/models/llama/modeling_llama.py\n",
        "def rotate_half(x: Tensor) -> Tensor:\n",
        "    \"\"\"Turns concat(x1, x2, dim=-1) into concat(-x2, x1, dim=-1).\"\"\"\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "\n",
        "# Copied from  https://github.com/huggingface/transformers/blob/7f95372c6267d3163fd2aa74aeff9d84ddb6cc35/src/transformers/models/llama/modeling_llama.py\n",
        "def apply_rotary_pos_emb(q: Tensor, k: Tensor, cos: Tensor, sin: Tensor) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\"Applies Rotary Position Embedding (RoPE) to the query and key tensors.\n",
        "\n",
        "    Args:\n",
        "        q: The query tensor, shape (batch_size, heads, seq_len, head_dim).\n",
        "        k: The key tensor, shape (batch_size, heads, seq_len, head_dim).\n",
        "        cos: The cosine part of the rotary embedding, shape (batch_size, seq_len, head_dim).\n",
        "        sin: The sine part of the rotary embedding, shape (batch_size, seq_len, head_dim).\n",
        "    Returns: (q, k) tuple after rotation.\n",
        "\n",
        "    \"\"\"\n",
        "    cos = cos.unsqueeze(1)  # Add the 'heads' dimension (1 but broadcastable to the actual number).\n",
        "    sin = sin.unsqueeze(1)\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "\n",
        "# modified version of https://github.com/huggingface/transformers/blob/7f95372c6267d3163fd2aa74aeff9d84ddb6cc35/src/transformers/models/llama/modeling_llama.py\n",
        "def custom_attention_forward(\n",
        "    self,\n",
        "    hidden_states: Tensor,\n",
        "    position_embeddings: tuple[Tensor, Tensor],\n",
        "    attention_mask: Tensor | None = None,\n",
        "    position_ids: Tensor | None = None,\n",
        "    past_key_values=None,\n",
        "    use_cache: bool = False,\n",
        "    cache_position: torch.LongTensor | None = None,\n",
        "    **kwargs,\n",
        "):\n",
        "    x = hidden_states\n",
        "    bsz, q_len, _ = x.shape\n",
        "\n",
        "    query_states = self.q_proj(x)\n",
        "    key_states = self.k_proj(x)\n",
        "    value_states = self.v_proj(x)\n",
        "\n",
        "    # Split last dim into num_heads & head_dim; then swap seq_len and num_head dims.\n",
        "    query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
        "    key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
        "    value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    # Apply RoPE.\n",
        "    cos, sin = position_embeddings\n",
        "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
        "\n",
        "    # Use KV-cache.\n",
        "    if past_key_values is not None:\n",
        "        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
        "        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
        "        key_states, value_states = past_key_values.update(\n",
        "            key_states, value_states, self.layer_idx, cache_kwargs\n",
        "        )\n",
        "\n",
        "    # Swap back num_head and seq_len dims.\n",
        "    query_states = query_states.transpose(1, 2)\n",
        "    key_states = key_states.transpose(1, 2)\n",
        "    value_states = value_states.transpose(1, 2)\n",
        "\n",
        "    attn_output = attention_forward(\n",
        "        query=query_states,\n",
        "        key=key_states,\n",
        "        value=value_states,\n",
        "        output_projection=self.o_proj,\n",
        "    )\n",
        "\n",
        "    return attn_output, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a3BEs6usjCc"
      },
      "source": [
        "### Testing\n",
        "You can briefly test your solution below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj3mVnnVsjCc",
        "outputId": "9d11e928-8480-4ad1-af71-b57f912dbd13"
      },
      "outputs": [],
      "source": [
        "for l in model.model.layers:\n",
        "    l.self_attn.forward = partial(custom_attention_forward, self=l.self_attn)\n",
        "\n",
        "text = [\"2 + 7 = \"]\n",
        "\n",
        "tokens_mask = tokenizer(text, return_tensors=\"pt\")\n",
        "tokens = tokens_mask[\"input_ids\"]\n",
        "attention_mask = tokens_mask[\"attention_mask\"]\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_ids=tokens.to(device))\n",
        "    next_token = torch.argmax(output.logits[0, -1])\n",
        "    print(next_token)\n",
        "    decoded = tokenizer.decode(next_token)\n",
        "    print(f\"Model answer: {decoded}\")\n",
        "    assert decoded == \"9\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkour94wsjCc",
        "outputId": "ad0b2f44-3936-4089-97b7-9e68a70d76bd"
      },
      "outputs": [],
      "source": [
        "## If you have implemented the attention that can handle token-by-token generation you can check your solution using the code below.\n",
        "\n",
        "text = \"Solve x + 3 = 7\"\n",
        "tokens_mask = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        inputs=tokens_mask[\"input_ids\"].to(device),\n",
        "        max_new_tokens=8,\n",
        "        num_beams=1,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    print(tokenizer.batch_decode(output))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
