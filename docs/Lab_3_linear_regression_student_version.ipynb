{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRgO4mhuPAZG"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Programu Operacyjnego Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s47n23ty7ss5"
      },
      "source": [
        "# Simple linear regression\n",
        "\n",
        "In this exercise you will train a linear regression model via gradient descent in the simplest scenario, i.e. recreating an affine function.\n",
        "\n",
        "The setup is as follows:\n",
        "* we are given a set of pairs $(x, y)$, where $x$ represents the feature, and $y$ is the target,\n",
        "* our hypothesis is $h(x) = ax + b$,\n",
        "* we will use the dataset consisting of set of pairs to figure out the right values for $a$ and $b$,\n",
        "* to do so we will optimize the loss function: $J(a,b) = \\frac{1}{n}\\sum_{i=1}^n (y_i - h(x_i))^2$,\n",
        "* with the loss function in hand we can improve our guesses iteratively:\n",
        "    * $a^{t+1} = a^t - \\text{step_size} \\cdot \\frac{\\partial J(a,b)}{\\partial a}$,\n",
        "    * $b^{t+1} = b^t - \\text{step_size} \\cdot \\frac{\\partial J(a,b)}{\\partial b}$,\n",
        "* we can end the process after some predefined number of epochs (or when the changes are no longer meaningful).\n",
        "\n",
        "Let's start with creating the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOm-6OMd7ss7"
      },
      "source": [
        "import random\n",
        "\n",
        "_a = 0.3\n",
        "_b = 0.5\n",
        "\n",
        "f = lambda x: _a * x + _b # ground truth\n",
        "g = lambda x: f(x) + random.gauss(0, 0.02) # a noisy version of f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EQLIz_Y7stB"
      },
      "source": [
        "n = 50 # number of examples\n",
        "\n",
        "xs = [random.random() for _ in range(n)] # features\n",
        "ys = list(map(g, xs)) # targets\n",
        "\n",
        "ts = list(map(f, xs)) # we don't get to see this"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyIWYrek7stF"
      },
      "source": [
        "Our goal is to recreate $f$. However, as reality can be harsh (and usually is) we only get to observe $g$. We observe it as a list of pairs $(x,y) \\in \\text{zip}(xs, ys)$.\n",
        "\n",
        "Let's plot the data. We will use the `plotly` library to make the plots interactive, which allows for easier inspection of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsP2nL2NPQYX"
      },
      "source": [
        "!pip install -q plotly==4.2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBp8TaLJ7stH"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(x=xs, y=ys)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRqybzzs7stM"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def mse_loss(ys, ps):\n",
        "    assert len(ys) == len(ps)\n",
        "\n",
        "    ### YOUR CODE BEGINS HERE ###\n",
        "    ### YOUR CODE ENDS HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofYLNJM87stQ"
      },
      "source": [
        "Please take a while to (roughly) guess the output before executing the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMavAdN77stR"
      },
      "source": [
        "mse_loss(ys, ts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTRbX1tm7stV"
      },
      "source": [
        "Let's now implement the algorithm\n",
        "\n",
        "Hint: To make sure that you correctly compute the gradients, you can compute them numerically and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChxX0fEL7stW"
      },
      "source": [
        "a = 0. # our initial guess for _a\n",
        "b = 0. # our initial guess for _b\n",
        "lr = 0.5 # step size\n",
        "\n",
        "n_epochs = 40 # number of passes over the training data\n",
        "\n",
        "def predict(a, b, xs=xs):\n",
        "    return [a * x + b for x in xs]\n",
        "\n",
        "def evaluate(a, b, xs=xs, ys=ys):\n",
        "    return mse_loss(ys, predict(a, b, xs))\n",
        "\n",
        "losses = [evaluate(a, b)]\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    #############################\n",
        "    # TODO: Fill in the details #\n",
        "    #############################\n",
        "    ### YOUR CODE BEGINS HERE ###\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    loss = evaluate(a, b)\n",
        "    losses.append(loss)\n",
        "\n",
        "    print(f'Iter: {i:>3} Loss: {loss:8.8f} a: {a:8.5f}, b: {b:8.5f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdR0279A7stZ"
      },
      "source": [
        "fig = px.line(y=losses, labels={'y':'loss'})\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7c_9F8u7stc"
      },
      "source": [
        "Let's now visually asses how we do on training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH34w_867std"
      },
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig = px.scatter(x=xs, y=ys)\n",
        "dense_x = np.linspace(np.min(xs), np.max(xs), 100)\n",
        "fig.add_trace(go.Scatter(x=dense_x, y=predict(a, b, dense_x), name='linear fit', mode='lines'))\n",
        "fig.add_trace(go.Scatter(x=xs, y=ts, name='y without noise', mode='markers'))\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlW25Mnq7sth"
      },
      "source": [
        "Let's check our implementation vs. the one in sklearn and numpy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r6GSafQ7sti"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.array(xs).reshape((len(xs), 1))\n",
        "regr = LinearRegression()\n",
        "regr.fit(X, ys) # training\n",
        "\n",
        "sk_a = float(regr.coef_)\n",
        "sk_b = regr.intercept_\n",
        "sk_loss = mse_loss(ys, regr.predict(X))\n",
        "\n",
        "print(f'Loss: {sk_loss:8.8f} a: {sk_a:8.5f}, b: {sk_b:8.5f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYTJMBV2eybu"
      },
      "source": [
        "z = np.polyfit(x=xs, y=ys, deg=1)\n",
        "print(z)\n",
        "f = np.poly1d(z)\n",
        "print(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GsLRyMxPIoU"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>"
      ]
    }
  ]
}