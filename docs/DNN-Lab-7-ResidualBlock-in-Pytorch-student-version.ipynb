{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcTwzhX8fBqs"
      },
      "source": [
        "In this exercise, we are going to implement a [ResNet-like](https://arxiv.org/pdf/1512.03385.pdf) architecture for the image classification task.\n",
        "The model is trained on the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "1. Implement residual connections in the missing places in the code.\n",
        "2. Check that the given implementation reaches 97% test accuracy after a few epochs.\n",
        "3. Check that when extending the residual blocks to 20 (having 40+ layers total), the model still trains well, i.e., achieves 97+% accuracy after three epochs.\n",
        "\n",
        "We recommend switching to GPU, after initial testing on CPU.\n",
        "With 20 residual blocks, one training epoch takes ~15 min on Colab CPU and ~30s on Colab GPU.\n",
        "Remember to \"disconnect and delete runtime\" when you finish using it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYAsziKffBFV"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import v2\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM62tq5FsOVi"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=out_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Input: shape (B, in_channels, H, W).\n",
        "        Output: shape (B, out_channels, H, W).\n",
        "        \"\"\"\n",
        "        # TODO: implement forward function\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtpIAcICsOVj"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_residual_blocks: int) -> None:\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(\n",
        "            ResidualBlock(1, 16),\n",
        "            *(ResidualBlock(16, 16) for _ in range(num_residual_blocks - 1)),\n",
        "        )\n",
        "        self.head = nn.Linear(28 * 28 * 16, 10)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Input shape: shape (B, 1, 28, 28).\n",
        "        Output shape: log probabilities, shape (B, 10).\n",
        "        \"\"\"\n",
        "        x = self.backbone(x)  # shape (B, 16, 28, 28)\n",
        "        x = nn.Flatten(start_dim=1)(x)  # shape (B, 28 * 28 * 16)\n",
        "        x = self.head(x)  # shape (B, 10)\n",
        "        output = nn.LogSoftmax(dim=1)(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMtap4QCfBH8"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: nn.Module,\n",
        "    device: torch.device,\n",
        "    train_loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    epoch: int,\n",
        "    log_interval: int,\n",
        ") -> None:\n",
        "    model.train()\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}\")\n",
        "    for batch_idx, (data, target) in progress_bar:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "\n",
        "def test(model: nn.Module, device: torch.device, test_loader: DataLoader) -> None:\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    test_set_size = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            test_set_size += data.shape[0]\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += (pred == target).sum().item()\n",
        "\n",
        "    test_loss /= test_set_size\n",
        "\n",
        "    print(f\"Test loss: {test_loss:.4f}, accuracy: {correct}/{test_set_size} ({correct / test_set_size:.1%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5GlMs1-fBKP",
        "outputId": "c48265b2-658e-494d-d9a8-ca4cf46b291e"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "test_batch_size = 1000\n",
        "epochs = 3\n",
        "lr = 1e-2\n",
        "seed = 1\n",
        "log_interval = 10\n",
        "\n",
        "# Check for CUDA / MPS (Apple) / XPU (Intel) / ... accelerator.\n",
        "# This does not detect XLA devices (Google TPUs), they'd need separate checks.\n",
        "device = torch.accelerator.current_accelerator(check_available=True) or torch.device(\"cpu\")\n",
        "use_accel = device != torch.device(\"cpu\")\n",
        "print(use_accel, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgfUP23AfBMd"
      },
      "outputs": [],
      "source": [
        "class DataloaderArgs(TypedDict, total=False):\n",
        "    batch_size: int\n",
        "    shuffle: bool\n",
        "    num_workers: int\n",
        "    pin_memory: bool\n",
        "\n",
        "train_kwargs: DataloaderArgs = {\"batch_size\": batch_size, \"num_workers\": 2, \"shuffle\": True, \"pin_memory\": use_accel}\n",
        "test_kwargs: DataloaderArgs = {\"batch_size\": test_batch_size, \"num_workers\": 2, \"pin_memory\": use_accel}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0KPoUtsfBOs",
        "outputId": "a85bd224-27dc-4e59-a3ff-1e44c73df47e"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(seed)\n",
        "\n",
        "transform = v2.Compose(\n",
        "    [\n",
        "        v2.ToImage(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.Normalize((0.1307,), (0.3081,)),\n",
        "    ]\n",
        ")\n",
        "train_dataset = datasets.MNIST(\n",
        "    \"../data\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_dataset = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, **train_kwargs)\n",
        "test_loader = DataLoader(test_dataset, **test_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezvIQbgsfBRT",
        "outputId": "2d5c94be-7a31-46f4-9e3c-0e2907b0d8e1"
      },
      "outputs": [],
      "source": [
        "model = Net(num_residual_blocks=2).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
        "    test(model, device, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
