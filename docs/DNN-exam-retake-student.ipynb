{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yt6ZsxXC-Enh",
    "jukit_cell_id": "3bbXhEGl0p"
   },
   "source": [
    "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
    "\n",
    "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
    "<hr>\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
    "\n",
    "<center>\n",
    "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
    "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
    "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
    "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ykx6csq-Enk",
    "jukit_cell_id": "MjFPoyxoBk"
   },
   "source": [
    "# Latent Space Classifier\n",
    "\n",
    "\n",
    "In this task, you will:\n",
    "* train a Variational AutoEncoder on MNIST (the code is already prepared and ready)\n",
    "* train a digit classifier on the latent space of the Variational AutoEncoder\n",
    "\n",
    "## Variational AutoEncoder - review\n",
    "Below is a quick reminder on the Variational AutoEncoder:\n",
    "\n",
    "* Let $P^*$ be the true data distribution. We have some samples from this.\n",
    "* Let $p(z)$ be a *prior* distribution over the latent space. In our model, it is a multivariate Gaussian distribution $N(0,\\mathbb{I})$.\n",
    "* Let $E(x)$ be the encoder that accepts data points as input and outputs distributions over the latent space $Z$. The produced distribution is denoted $q_\\phi(z|x)$ and is the (approximate) *posterior* distribution. In our model, this is multivariate Gaussian distribution $q_\\phi(z|x) \\sim N(\\mu, diag(\\sigma^2))$, where:\n",
    "    1. $\\phi$ are weights of the encoder network.\n",
    "    2. The Encoder network accepts data points as input and outputs $\\mu$ and $\\sigma$, which are vectors of the same length as latent space. They are used to construct the approximate posterior distribution $q_\\phi(z|x)$.\n",
    "* Let $D(z)$ be the decoder that accepts samples from the latent distribution and output parameters of the likelihood distribution $p_\\theta(x|z)$. In our model, this is the Bernoulli trial per each pixel $p_\\theta(x|z_0) \\sim Bern(p)$, where:\n",
    "    1. $\\theta$ are weights of the decoder network.\n",
    "    2. The decoder network accepts a sample from the posterior distribution $q_\\phi(z|x)$ and outputs p, which is a matrix of the shape of the input image. Each value of the matrix is the parameter $\\pi$ of the Bernoulli trial $Bern(\\pi)$ for the corresponding pixel.\n",
    "    3. Data points are clipped to only contain values 0 and 1 so that the model can be trained in the given setup.\n",
    "\n",
    "The Variational AutoEncoder works by maximizing the Evidence Lower Bound (ELBO):\n",
    "\n",
    "$$ELBO = \\mathbb{E}_{z \\sim q(z|x)} \\big[\\log p_\\theta(x|z)\\big] - \\mathbb{KL}\\big(q_\\phi(z | x) || p(z)\\big).$$\n",
    "\n",
    "Where the first term of the loss is trained via stochastic gradient descent. Whereas, the second term can be calculated analytically in our setup and is equal to the following:\n",
    "\n",
    "$$ \\mathbb{KL}\\big( \\mathcal{N}(\\mu, \\sigma^2) || \\mathcal{N}(0, 1) \\big) = \\frac12 \\big(\\sigma^2 - \\log(\\sigma^2) + \\mu^2 - 1 \\big).$$\n",
    "\n",
    "You do not need to use the formulas above, as the Variational AutoEncoder is already implemented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational AutoEncoder - code\n",
    "The code for VAE is already completed and attached below. Run the code to train the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5oaegYX3-Enm",
    "jukit_cell_id": "mIRhZXmXjZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms # type: ignore\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CG4DPj87-Enn",
    "jukit_cell_id": "o6ghXwN1P4"
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "test_batch_size = 1000\n",
    "epochs = 5\n",
    "lr = 5e-3\n",
    "seed = 1\n",
    "log_interval = 5\n",
    "latent_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6_fkxSGM-Enn",
    "jukit_cell_id": "1fDwZIPWsn",
    "outputId": "2e99c667-72ee-4bd5-f1a6-762e45067c78"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                    'pin_memory': True,\n",
    "                    'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "J97S4jqt-Enp",
    "jukit_cell_id": "LHCziTTTb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 573139.812500\n",
      "Train Epoch: 1 [5120/60000 (8%)]\tLoss: 211915.125000\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 183882.062500\n",
      "Train Epoch: 1 [15360/60000 (25%)]\tLoss: 170493.437500\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 160695.531250\n",
      "Train Epoch: 1 [25600/60000 (42%)]\tLoss: 154435.843750\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 146601.812500\n",
      "Train Epoch: 1 [35840/60000 (59%)]\tLoss: 142021.406250\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 139421.703125\n",
      "Train Epoch: 1 [46080/60000 (76%)]\tLoss: 134739.140625\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 133259.656250\n",
      "Train Epoch: 1 [56320/60000 (93%)]\tLoss: 132427.562500\n",
      "\n",
      "Test set: Average loss: 129173.3984\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 130287.820312\n",
      "Train Epoch: 2 [5120/60000 (8%)]\tLoss: 128907.007812\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 128754.695312\n",
      "Train Epoch: 2 [15360/60000 (25%)]\tLoss: 125696.226562\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 126032.429688\n",
      "Train Epoch: 2 [25600/60000 (42%)]\tLoss: 125619.656250\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 123582.000000\n",
      "Train Epoch: 2 [35840/60000 (59%)]\tLoss: 123792.156250\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 121677.710938\n",
      "Train Epoch: 2 [46080/60000 (76%)]\tLoss: 120663.937500\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 120914.289062\n",
      "Train Epoch: 2 [56320/60000 (93%)]\tLoss: 121561.539062\n",
      "\n",
      "Test set: Average loss: 116425.4844\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 118777.593750\n",
      "Train Epoch: 3 [5120/60000 (8%)]\tLoss: 119027.843750\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 117882.765625\n",
      "Train Epoch: 3 [15360/60000 (25%)]\tLoss: 119511.093750\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 116887.671875\n",
      "Train Epoch: 3 [25600/60000 (42%)]\tLoss: 116376.546875\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 117743.007812\n",
      "Train Epoch: 3 [35840/60000 (59%)]\tLoss: 115801.656250\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 115846.062500\n",
      "Train Epoch: 3 [46080/60000 (76%)]\tLoss: 114556.289062\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 117696.976562\n",
      "Train Epoch: 3 [56320/60000 (93%)]\tLoss: 113615.140625\n",
      "\n",
      "Test set: Average loss: 112812.0703\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 116242.000000\n",
      "Train Epoch: 4 [5120/60000 (8%)]\tLoss: 114736.257812\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 115447.250000\n",
      "Train Epoch: 4 [15360/60000 (25%)]\tLoss: 111841.640625\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 113159.218750\n",
      "Train Epoch: 4 [25600/60000 (42%)]\tLoss: 111823.625000\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 113444.351562\n",
      "Train Epoch: 4 [35840/60000 (59%)]\tLoss: 113296.281250\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 112390.734375\n",
      "Train Epoch: 4 [46080/60000 (76%)]\tLoss: 114106.859375\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 111101.218750\n",
      "Train Epoch: 4 [56320/60000 (93%)]\tLoss: 112410.789062\n",
      "\n",
      "Test set: Average loss: 109207.0000\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 112659.546875\n",
      "Train Epoch: 5 [5120/60000 (8%)]\tLoss: 111500.406250\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 111923.054688\n",
      "Train Epoch: 5 [15360/60000 (25%)]\tLoss: 111412.078125\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 112756.765625\n",
      "Train Epoch: 5 [25600/60000 (42%)]\tLoss: 111700.250000\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 110586.359375\n",
      "Train Epoch: 5 [35840/60000 (59%)]\tLoss: 112798.289062\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 108556.265625\n",
      "Train Epoch: 5 [46080/60000 (76%)]\tLoss: 108901.195312\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 110845.890625\n",
      "Train Epoch: 5 [56320/60000 (93%)]\tLoss: 109016.648438\n",
      "\n",
      "Test set: Average loss: 107282.6953\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Binarize:\n",
    "    def __call__(self, sample: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.bernoulli(sample)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), Binarize()])\n",
    "dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "train_loader = DataLoader(dataset1, **train_kwargs)\n",
    "test_loader = DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "EncoderOutput = namedtuple(\"EncoderOutput\", [\"mu\", \"sigma\"])\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, linear_sizes: list[int], latent_size: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for in_layer_size, out_layer_size in zip(linear_sizes, linear_sizes[1:]):\n",
    "            self.layers.append(nn.Linear(in_layer_size, out_layer_size))\n",
    "            self.layers.append(nn.BatchNorm1d(out_layer_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "        self.last_layer_mu = nn.Linear(linear_sizes[-1], latent_size)\n",
    "        self.last_layer_sigma = nn.Linear(linear_sizes[-1], latent_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = nn.Flatten()(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        mu = self.last_layer_mu(x)\n",
    "        logsigma = self.last_layer_sigma(x)\n",
    "        return EncoderOutput(mu, torch.log(1 + torch.exp(logsigma)))\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, linear_sizes: list[int], output_size: tuple[int]):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for in_layer_size, out_layer_size in zip(linear_sizes, linear_sizes[1:]):\n",
    "            self.layers.append(nn.Linear(in_layer_size, out_layer_size))\n",
    "            self.layers.append(nn.BatchNorm1d(out_layer_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Linear(linear_sizes[-1], output_size[0] * output_size[1]), nn.Sigmoid()\n",
    "        )\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            z = layer(z)\n",
    "\n",
    "        x = self.last_layer(z)\n",
    "\n",
    "        x = x.view(-1, 1, *self.output_size)\n",
    "        return x\n",
    "\n",
    "\n",
    "VariationalAutoEncoderOutput = namedtuple(\n",
    "    \"VariationalAutoEncoderOutput\", [\"mu\", \"sigma\", \"p\"]\n",
    ")\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_linear_sizes: list[int],\n",
    "        latent_size: int,\n",
    "        decoder_linear_sizes: list[int],\n",
    "        output_size: tuple[int],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(encoder_linear_sizes, latent_size)\n",
    "        self.decoder = Decoder(decoder_linear_sizes, output_size)\n",
    "        self.latent_size = latent_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        encoded = self.encoder(x)\n",
    "\n",
    "        z = torch.normal(0.0, 1.0, size=list(encoded.mu.size())).to(device)\n",
    "        z = (z * encoded.sigma) + encoded.mu\n",
    "\n",
    "        decoded = self.decoder(z)\n",
    "        return VariationalAutoEncoderOutput(encoded.mu, encoded.sigma, decoded)\n",
    "\n",
    "    def sample_latent(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        encoded = self.encoder(x)\n",
    "        z = torch.normal(0.0, 1.0, size=list(encoded.mu.size())).to(device)\n",
    "        z = (z * encoded.sigma) + encoded.mu\n",
    "\n",
    "        return z\n",
    "\n",
    "    def sample(self, sample_size: int, samples=None) -> torch.Tensor:\n",
    "        if samples is None:\n",
    "            samples = torch.normal(0.0, 1.0, size=(sample_size, self.latent_size)).to(\n",
    "                device\n",
    "            )\n",
    "\n",
    "        decoded = self.decoder(samples)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def KL_gaussian_loss(mu, sigma):\n",
    "    return torch.sum(((sigma * sigma) - (2 * torch.log(sigma)) + (mu * mu) - 1) / 2)\n",
    "\n",
    "\n",
    "def ELBO(x, p, mu, sigma):\n",
    "    BCE = F.binary_cross_entropy(p, x, reduction=\"sum\")\n",
    "    KL = KL_gaussian_loss(mu, sigma)\n",
    "    return BCE + KL\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    device: torch.device,\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    epoch: int,\n",
    "    log_interval: int,\n",
    "):\n",
    "    model.train()\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = ELBO(data, output.p, output.mu, output.sigma)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test(model: nn.Module, device: torch.device, test_loader: DataLoader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            loss = ELBO(data, output.p, output.mu, output.sigma)\n",
    "            test_loss = test_loss + (loss * data.size(0))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\"\\nTest set: Average loss: {:.4f}\\n\".format(test_loss))\n",
    "\n",
    "\n",
    "vae = VariationalAutoEncoder(\n",
    "    [28 * 28, 500, 350], latent_size, [latent_size, 350, 500], (28, 28)\n",
    ")\n",
    "vae.to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(vae, device, train_loader, optimizer, epoch, log_interval)\n",
    "    test(vae, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Latent Classifier - subtasks:\n",
    "Below are all graded subtasks associated with this exam task:\n",
    "\n",
    "1. Complete the implementation of `ClassificationHead`, which, given a latent vector generated by an `Encoder` for an image $i$ (just the $\\mu$ part), predicts to which class (digit) the image $i$ belongs.\n",
    "2. Complete the implementation of `Classifier`, which, given an input image, first encodes it with a frozen pre-trained `Encoder` and then passes it through the `ClassificationHead` to generate logits for classification.\n",
    "3. Complete the implementation of `train_classifier`, which trains the `Classifier` module. To be more precise, it trains only the `ClassificationHead` of the `Classifier`. So, for an image $i$, given the output of the pre-trained `Encoder` on the image $i$ (just the $\\mu$ part), `ClassificationHead` predicts the class to which the image belongs (which digit is present on the image).\n",
    "\n",
    "Remarks:\n",
    "* To earn all points, your model should achieve an accuracy greater than 90% (see test at the end).\n",
    "* Note that not all variables should be trained, and in particular, no gradients should be propagated throughout the `Encoder`.\n",
    "* Use a proper loss function for training the `Classifier` and select appropriate training parameters to ensure the final accuracy is above 90%.\n",
    "* Do not change the code outside the following blocks \n",
    "```python3\n",
    "#### TODO ####\n",
    "\n",
    "##############\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifactionHead(nn.Module):  \n",
    "    def __init__(self, latent_size, num_classes):\n",
    "        super().__init__()\n",
    "        #### TODO ####\n",
    "        pass\n",
    "        ##############\n",
    "\n",
    "    def forward(self, x):\n",
    "        #### TODO ####\n",
    "        pass\n",
    "        ##############\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):  # 1pt\n",
    "    def __init__(self, vae, head):\n",
    "        super().__init__()\n",
    "        #### TODO ####\n",
    "        pass\n",
    "        ##############\n",
    "\n",
    "    def forward(self, x):\n",
    "        #### TODO ####\n",
    "        pass\n",
    "        ##############\n",
    "\n",
    "\n",
    "def train_classifier(train_loader, epochs=10, **kwargs):\n",
    "    #### TODO ####\n",
    "    pass\n",
    "    ##############\n",
    "\n",
    "#### TODO ####\n",
    "# Adjust kwargs if needed\n",
    "train_function_kwargs = {}\n",
    "##############\n",
    "\n",
    "classifier = train_classifier(train_loader, **train_function_kwargs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(classifier):\n",
    "    classifier.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (data, label) in test_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            output = classifier(data)\n",
    "            loss = torch.mean((torch.argmax(output, dim=-1) == label).to(float))\n",
    "            test_loss = test_loss + (loss * data.size(0))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n",
    "    return test_loss\n",
    "\n",
    "assert test_classifier(classifier) > 0.90, 'Classifier not trained well enough'\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
